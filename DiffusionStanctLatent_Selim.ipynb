{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c90751a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import signal\n",
    "signal.signal(signal.SIGINT, signal.SIG_DFL)\n",
    "\n",
    "import os\n",
    "os.environ[\"TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataload import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b58896",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d12bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(datset, num_samples=20, cols=4):\n",
    "    \"\"\" Plots some samples from the dataset \"\"\"\n",
    "    plt.figure(figsize=(15,15)) \n",
    "    for i, img in enumerate(data):\n",
    "        if i == num_samples:\n",
    "            break\n",
    "        plt.subplot(int(num_samples/cols) + 1, cols, i + 1)\n",
    "        plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed52ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\" \n",
    "    Takes an image and a timestep as input and \n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6a374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define beta schedule\n",
    "T = 1000\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d944e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c21dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed05683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3833f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 128+128\n",
    "        down_channels = (256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256)\n",
    "        out_dim = 1\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], 128, out_dim)\n",
    "\n",
    "    def forward(self, x, y_0, timestep): # Latent_nose + latent_non-standard img    \n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # concating conditioned image with the noisey imag        \n",
    "        x = torch.cat((x, y_0), dim=1) # Noise + non-standard img        \n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e12fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "  return nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "    nn.ReLU(inplace=True),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9307e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetED(nn.Module):\n",
    "  def __init__(self, n_class=3):\n",
    "    super().__init__()\n",
    "    self.base_model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    self.base_layers = list(self.base_model.children())\n",
    "\n",
    "    self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "    self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "    self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "    self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "    self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "    self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "\n",
    "\n",
    "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "    self.conv_original_size2 = convrelu(128, 64, 3, 1)\n",
    "\n",
    "    self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def diffusion_noise_2_img(self, x, noise, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * noise / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "\n",
    "  def forward(self, input, diff_model=None, y_0=None, t=None, device=None):\n",
    "\n",
    "    noise = None\n",
    "    noise_pred=None\n",
    "\n",
    "    layer0 = self.layer0(input)\n",
    "    layer1 = self.layer1(layer0)\n",
    "    layer2 = self.layer2(layer1)\n",
    "\n",
    "\n",
    "    x = self.layer2_1x1(layer2)\n",
    "\n",
    "    if diff_model :\n",
    "        #x = lat_diff(x)\n",
    "        y_lat = self.layer2_1x1(self.layer2(self.layer1(self.layer0(y_0))))\n",
    "        y_noisy, noise = forward_diffusion_sample(y_lat, t, device)\n",
    "        #print(\"My input lat shape \",y_lat.shape, x.shape, t )\n",
    "        noise_pred = diff_model(y_noisy, x, t)   # this part is for the new model \n",
    "        x = self.diffusion_noise_2_img( x, noise_pred, t)\n",
    "\n",
    "    x = self.upsample(x)\n",
    "    #x = torch.cat([x, x_original], dim=1)\n",
    "    x = self.conv_original_size2(x)\n",
    "    x = self.upsample(x)\n",
    "    x = self.conv_original_size1(x)\n",
    "    x = self.upsample(x)\n",
    "    out = torch.tanh( self.conv_last(x) ), \n",
    "\n",
    "    return out, noise, noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73cfae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  59366304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleUnet(\n",
       "  (time_mlp): Sequential(\n",
       "    (0): SinusoidalPositionEmbeddings()\n",
       "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (downs): ModuleList(\n",
       "    (0): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=1024, bias=True)\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (output): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleUnet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac63ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, y_0, t):\n",
    "    y_noisy, noise = forward_diffusion_sample(y_0, t, device)\n",
    "\n",
    "    noise_pred = model(y_noisy, x_0.to(device), t)   # this part is for the new model  \n",
    "\n",
    "    return F.l1_loss(noise, noise_pred) #+ F.l1_loss(x_0.to(device), x_noisy-noise_pred)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, y_0, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, y_0, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image(x_test_0, y_test_0, t, epoch, img_id):\n",
    "    img_size = IMG_SIZE\n",
    "    \n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device) #imgs[0].unsqueeze(0) #\n",
    "    y_test_0 = y_test_0[0].unsqueeze(0).to(device)  # take a sample from the batch \n",
    "    x_test_0 = x_test_0[0].unsqueeze(0).to(device)  # take a sample from the batch \n",
    "    #print(img.shape)    \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "\n",
    "    plt.subplot(1, num_images+2, 1)\n",
    "    show_tensor_image(y_test_0.detach().cpu())\n",
    "\n",
    "    plt.subplot(1, num_images+2, 2)\n",
    "    show_tensor_image(x_test_0.detach().cpu())\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long) # it create a tensor with size 1 with value i\n",
    "        img = sample_timestep(img, x_test_0, t) # get the dennoised with timestep t=i\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images+2, int(i/stepsize)+3)\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "\n",
    "    plt.savefig(\"results_stanct/validation/\"+str(epoch)+'_'+str(img_id)+'.png', format='png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085fa17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "10 10\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "data_dir=r\"C:\\Users\\dsi224\\Documents\\PythonFiles\\PythonCodesForDiffusionModel\\SE\\SE\"  \n",
    "kernel_label = {0:\"Bl64\", 1:\"Br40\", 2:\"B70f\", 3:\"B31f\", 4:\"L\", 5:\"B\", 6:\"STANDARD\", 7:\"LUNG\"}\n",
    "\n",
    "\n",
    "global_dataset =  torch.utils.data.DataLoader(\\\n",
    "\t\tGlobalTrainDataset(data_dir, kernel = \"all\", limit=-1), \\\n",
    "\t\tbatch_size=BATCH_SIZE, \\\n",
    "\t\tshuffle=True)\n",
    "\n",
    "train_dataset =  torch.utils.data.DataLoader(\\\n",
    "\t\tPairTrainDataset(data_dir, kernel_A = \"BR40\", kernel_B = \"BL64\", limit=-1), \\\n",
    "\t\tbatch_size=BATCH_SIZE, \\\n",
    "\t\tshuffle=True)\n",
    "\n",
    "test_dataset =  torch.utils.data.DataLoader(\\\n",
    "\t\tPairTrainDataset(data_dir, kernel_A = \"BR40\", kernel_B = \"BL64\", limit=5), \\\n",
    "\t\tbatch_size=1, \\\n",
    "\t\tshuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec88103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \".\"\n",
    "\n",
    "os.makedirs(train_dir+\"/validation/\")\n",
    "os.makedirs(train_dir+\"/checkpoint/\")\n",
    "os.makedirs(train_dir+'/checkpoint/latest_global_net.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d1993f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Global model training ...\n",
      " Global Epoch 20 step 0: Loss L1 0.7178987860679626\n",
      " Global Epoch 21 step 0: Loss L1 0.5748395323753357\n",
      " Global Epoch 22 step 0: Loss L1 0.14957760274410248\n",
      " Global Epoch 23 step 0: Loss L1 0.09766489267349243\n",
      " Global Epoch 24 step 0: Loss L1 0.06287359446287155\n",
      " Global Epoch 25 step 0: Loss L1 0.08443065732717514\n",
      " Global Epoch 26 step 0: Loss L1 0.040322743356227875\n",
      " Global Epoch 27 step 0: Loss L1 0.07599335163831711\n",
      " Global Epoch 28 step 0: Loss L1 0.08658187091350555\n",
      " Global Epoch 29 step 0: Loss L1 0.06543809175491333\n",
      " Global Epoch 30 step 0: Loss L1 0.06653542071580887\n",
      " Global Epoch 31 step 0: Loss L1 0.03524135425686836\n",
      " Global Epoch 32 step 0: Loss L1 0.07565223425626755\n",
      " Global Epoch 33 step 0: Loss L1 0.06490573287010193\n",
      " Global Epoch 34 step 0: Loss L1 0.0726899802684784\n",
      " Global Epoch 35 step 0: Loss L1 0.07228423655033112\n",
      " Global Epoch 36 step 0: Loss L1 0.02661641500890255\n",
      " Global Epoch 37 step 0: Loss L1 0.06663317233324051\n",
      " Global Epoch 38 step 0: Loss L1 0.05717599391937256\n",
      " Global Epoch 39 step 0: Loss L1 0.061847783625125885\n",
      " Global Epoch 40 step 0: Loss L1 0.06190841645002365\n",
      " Global Epoch 41 step 0: Loss L1 0.0620000958442688\n",
      " Global Epoch 42 step 0: Loss L1 0.06220920383930206\n",
      " Global Epoch 43 step 0: Loss L1 0.024394487962126732\n",
      " Global Epoch 44 step 0: Loss L1 0.06802533566951752\n",
      " Global Epoch 45 step 0: Loss L1 0.06837378442287445\n",
      " Global Epoch 46 step 0: Loss L1 0.022491609677672386\n",
      " Global Epoch 47 step 0: Loss L1 0.059648316353559494\n",
      " Global Epoch 48 step 0: Loss L1 0.022319164127111435\n",
      " Global Epoch 49 step 0: Loss L1 0.02750476822257042\n",
      " Global Epoch 50 step 0: Loss L1 0.06616795063018799\n",
      " Global Epoch 51 step 0: Loss L1 0.022983629256486893\n",
      " Global Epoch 52 step 0: Loss L1 0.05887409672141075\n",
      " Global Epoch 53 step 0: Loss L1 0.06161823496222496\n",
      " Global Epoch 54 step 0: Loss L1 0.019471583887934685\n",
      " Global Epoch 55 step 0: Loss L1 0.023321732878684998\n",
      " Global Epoch 56 step 0: Loss L1 0.0613270066678524\n",
      " Global Epoch 57 step 0: Loss L1 0.057784780859947205\n",
      " Global Epoch 58 step 0: Loss L1 0.05137792229652405\n",
      " Global Epoch 59 step 0: Loss L1 0.018791690468788147\n",
      " Global Epoch 60 step 0: Loss L1 0.05809469148516655\n",
      " Global Epoch 61 step 0: Loss L1 0.01816372014582157\n",
      " Global Epoch 62 step 0: Loss L1 0.021348943933844566\n",
      " Global Epoch 63 step 0: Loss L1 0.05730561539530754\n",
      " Global Epoch 64 step 0: Loss L1 0.06701286882162094\n",
      " Global Epoch 65 step 0: Loss L1 0.05676387995481491\n",
      " Global Epoch 66 step 0: Loss L1 0.018795598298311234\n",
      " Global Epoch 67 step 0: Loss L1 0.06829608976840973\n",
      " Global Epoch 68 step 0: Loss L1 0.01779279112815857\n",
      " Global Epoch 69 step 0: Loss L1 0.01668475940823555\n",
      " Global Epoch 70 step 0: Loss L1 0.015980642288923264\n",
      " Global Epoch 71 step 0: Loss L1 0.06629366427659988\n",
      " Global Epoch 72 step 0: Loss L1 0.0499892383813858\n",
      " Global Epoch 73 step 0: Loss L1 0.0205550417304039\n",
      " Global Epoch 74 step 0: Loss L1 0.021090790629386902\n",
      " Global Epoch 75 step 0: Loss L1 0.016451120376586914\n",
      " Global Epoch 76 step 0: Loss L1 0.020397786051034927\n",
      " Global Epoch 77 step 0: Loss L1 0.049077849835157394\n",
      " Global Epoch 78 step 0: Loss L1 0.014981541782617569\n",
      " Global Epoch 79 step 0: Loss L1 0.0620453767478466\n",
      " Global Epoch 80 step 0: Loss L1 0.018133899196982384\n",
      " Global Epoch 81 step 0: Loss L1 0.014313059858977795\n",
      " Global Epoch 82 step 0: Loss L1 0.06490471214056015\n",
      " Global Epoch 83 step 0: Loss L1 0.014135535806417465\n",
      " Global Epoch 84 step 0: Loss L1 0.06088055297732353\n",
      " Global Epoch 85 step 0: Loss L1 0.05095290392637253\n",
      " Global Epoch 86 step 0: Loss L1 0.06135501712560654\n",
      " Global Epoch 87 step 0: Loss L1 0.05490690469741821\n",
      " Global Epoch 88 step 0: Loss L1 0.019182557240128517\n",
      " Global Epoch 89 step 0: Loss L1 0.05659446865320206\n",
      " Global Epoch 90 step 0: Loss L1 0.05056323856115341\n",
      " Global Epoch 91 step 0: Loss L1 0.06135783717036247\n",
      " Global Epoch 92 step 0: Loss L1 0.05038142949342728\n",
      " Global Epoch 93 step 0: Loss L1 0.013659960590302944\n",
      " Global Epoch 94 step 0: Loss L1 0.05414748191833496\n",
      " Global Epoch 95 step 0: Loss L1 0.05400187894701958\n",
      " Global Epoch 96 step 0: Loss L1 0.05432376265525818\n",
      " Global Epoch 97 step 0: Loss L1 0.015725798904895782\n",
      " Global Epoch 98 step 0: Loss L1 0.018554244190454483\n",
      " Global Epoch 99 step 0: Loss L1 0.05379384011030197\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '_checkpoint/latest_global_net.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \t\t\t\u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# want to test only a single image\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \t\t\u001b[38;5;66;03m#break\t\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \t\u001b[38;5;66;03m#torch.save(ED_model.state_dict(), train_dir+'/checkpoint/latest_global_net.pt')\t\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \t\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mED_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_checkpoint/latest_global_net.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m ED_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\test_env\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\.conda\\envs\\test_env\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\.conda\\envs\\test_env\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '_checkpoint/latest_global_net.pt'"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_dir = \".\"\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(train_dir+\"validation/\")\n",
    "    os.makedirs(train_dir+\"checkpoint/\")\n",
    "\n",
    "\t\n",
    "f_log = open(train_dir+'log.txt', 'w')\n",
    "\n",
    "\n",
    "###################################### Defining models\n",
    "#Encoder-decoder model\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "ED_model = ResNetED().to(device)\n",
    "\n",
    "#ED_model.load_state_dict(torch.load(train_dir+'/checkpoint/latest_global_net.pt'))\n",
    "\n",
    "\n",
    "ED_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, ED_model.parameters()), lr=learning_rate)\n",
    "\n",
    "\n",
    "if True:\n",
    "\tprint('In Global model training ...')\n",
    "\n",
    "\tfor epoch in range(0, 100):\n",
    "\t\tfor idx, data in enumerate(global_dataset, 0):\n",
    "\t\t\tX, label = data  #non-STD, STD, glcm_STD, cls\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\t\n",
    "\t\t\tout= ED_model(X)[0][0]\n",
    "\t\t\t#print(out[0].shape)\n",
    "\t\t\tloss =  F.l1_loss(out, X)\n",
    "\t\t\n",
    "\t\t\t# Backpropagation based on the loss\n",
    "\t\t\tED_optimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tED_optimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tif idx%100==0:\n",
    "\t\t\t\tprint(' Global Epoch {} step {}: Loss L1 {}'.format(epoch, idx, loss))\n",
    "\t\t\t\tf_log.write(' Global Epoch {} step {}: Loss L1 {}'.format(epoch, idx, loss))\n",
    "\t\t\t\tf_log.write('\\n')\t\t\t\n",
    "\t\t\t#break\n",
    "\t\t\t\n",
    "\t\tif epoch%2==0:\n",
    "\t\t\ttorch.save(ED_model.state_dict(), train_dir+'/checkpoint/'+str(epoch)+'_global_net.pt')\n",
    "\n",
    "\t\n",
    "\t\t\n",
    "\t\t# test the model for model validation\t\n",
    "\t\tfor _idx, _data in enumerate(global_dataset, 0):\n",
    "\t\t\txs, label = _data  #non-STD, STD, glcm_STD, cls\n",
    "\t\t\txs = xs.to(device)\n",
    "\t\t\t\n",
    "\t\t\txs.requires_grad = False\t\t\n",
    "\t\t\t# Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "\t\t\t\n",
    "\t\t\twith torch.no_grad():\t\n",
    "\t\t\t\tout = ED_model(xs)[0][0]\n",
    "\t\t\t\t\n",
    "\t\t\tout = out.squeeze().cpu()\t\n",
    "\t\t\t\n",
    "\t\t\tout = torch.cat((out[0],xs.squeeze().cpu()[0]\t), axis=1)\t\t\t\n",
    "\t\t\tsave_image( (out * 0.5 + 0.5), train_dir+'/validation/'+str(epoch)+kernel_label[label.numpy()[0]]+'.png') #[-1, 1 ==> [0,1]]\t\t\t\t\n",
    "\t\t\tbreak # want to test only a single image\n",
    "\t\t#break\t\n",
    "\ttorch.save(ED_model.state_dict(), train_dir+'/checkpoint/latest_global_net.pt')\t\n",
    "\n",
    "\n",
    "ED_model.eval()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100 # Try more!\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "      optimizer.zero_grad()\n",
    "      #batch = batch.to(device)#print(b)\n",
    "      b, c, h, w= batch[0].shape\n",
    "\n",
    "      t = torch.randint(0, T, (b,), device=device).long()\n",
    "      #print(batch.shape)    \n",
    "      #break\n",
    "\t\t\n",
    "      out, noise, noise_pred = ED_model( batch[0].to(device), model, batch[1].to(device), t, device)\n",
    "      loss = F.l1_loss(noise_pred, noise) + F.l1_loss(out[0], batch[1].to(device))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if epoch % 5 == 0 and step == 0:\n",
    "        print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "        f_log.write(' Diffusion Epoch {} step {}: Loss L1 {}'.format(epoch, idx, loss))\n",
    "        f_log.write('\\n')\n",
    "\t\t\t\t\n",
    "        t = torch.randint(0, T, (1,), device=device).long()\t\n",
    "\t\t\n",
    "        for im_id, test_data in enumerate(test_dataset):\n",
    "            #test_data = test_data.to(device)#print(b)\n",
    "            #sample_plot_image(test_data[0], test_data[1], t, epoch, im_id)\n",
    "            out, noise, noise_pred = ED_model( test_data[0].to(device), model, test_data[1].to(device), t, device)\t\t\t\n",
    "            out = out[0].squeeze().cpu()\n",
    "            out = torch.cat((out, test_data[1].squeeze()\t), axis=1)\t\t\t\n",
    "            save_image( (out * 0.5 + 0.5), train_dir+'/validation/'+str(epoch)+'lat_diff.png') #[-1, 1 ==> [0,1]]\n",
    "            break\n",
    "      #break\n",
    "    #break\n",
    "\t\t\t\n",
    "print(\"Training Done!\")       \n",
    "torch.save(model.state_dict(), train_dir+'/checkpoint/latest_diffusion_net.pt')\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
